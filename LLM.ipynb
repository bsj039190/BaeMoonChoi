{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsj03\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4206,  0.3756, -0.0236,  0.2688]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 로딩\n",
    "model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=4)\n",
    "\n",
    "# 모델의 마지막 레이어를 회귀용으로 수정\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 4)  # 4개의 감정 (기쁨, 슬픔, 분노, 사랑)\n",
    "\n",
    "# 모델 학습을 위한 optimizer 및 loss function 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.MSELoss()  # 회귀이므로 MSE Loss 사용\n",
    "\n",
    "# 입력 텍스트\n",
    "text = \"오늘 날씨가 너무 좋다!\"\n",
    "\n",
    "# 토크나이저\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 텍스트 토큰화\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 모델 예측 (각 감정에 대한 점수를 예측)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 예측값\n",
    "logits = outputs.logits  # 4개의 감정에 대한 점수 예측\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0207,  0.2367, -0.1428, -0.0882]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
